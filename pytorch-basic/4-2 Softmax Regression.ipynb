{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소프트맥스 회귀의 비용 함수 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d6f3b0b210>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. low-level로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "# 소프트맥스 회귀를 거친 후 값 확인\n",
    "hypothesis = F.softmax(z, dim=0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스 회귀를 거친 후 0과 1사이의 값을 가지는 벡터로 변환되었다. 모든 원소의 합은 1이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim=1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 x 5 임의로 선언하여 소프트맥스 회귀를 적용하였다. 이 때, dim은 소프트맥스 회귀를 적용하는 차원을 의미한다. 각 행을 하나의 샘플데이터로 보고 소프트맥스 회귀를 적용해야 하므로 두 번째 차원에 적용하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 3, 2]) torch.LongTensor\n",
      "tensor([2, 3, 2]) torch.LongTensor\n",
      "tensor([1, 2], dtype=torch.int32) torch.IntTensor\n",
      "tensor([1, 2]) torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3, )).long()\n",
    "print(y, y.type())\n",
    "\n",
    "y_ = torch.randint(5, (3, ))\n",
    "print(y_, y_.type())\n",
    "\n",
    "y_ = torch.IntTensor([1, 2])\n",
    "print(y_, y_.type())\n",
    "\n",
    "y_ = torch.IntTensor([1, 2]).long()\n",
    "print(y_, y_.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5개의 클래스 중 임의로 3개의 레이블을 만든다. long() 함수는 텐서의 타입을 LongTensor로 변경하는 역할을 수행한다. randint() 함수의 출력은 이미 LongTensor 타입으로 바뀌는 것은 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [3],\n",
       "        [2]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scatter(dim, index, value)` 순서로 두 번째 차원으로 연산을 수행하고 값 1을 넣겠다는 의미. index의 값은 두 번째 차원의 값을 의미하기 때문에 3 x 5행렬에서는 5보다 작은 0 ~ 4의 값을 가져야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4779, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = ((y_one_hot * -torch.log(hypothesis)).sum(dim=1)).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 부호를 앞에 붙였을 때랑 뒤에 붙였을 때 grad_fn이 MeanBackward / NegBackward로 변경되는 것을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. high-level로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 결과에 로그를 씌울 때 softmax 의 출력값을 log의 입력값으로 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4661, -1.3655, -2.1714, -1.7770, -1.4664],\n",
       "        [-1.2339, -1.2855, -1.9580, -1.9562, -1.8985],\n",
       "        [-1.6426, -1.2071, -2.0212, -1.4390, -1.9821]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)\n",
    "torch.log(F.softmax(z, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 과정을 결합한 log_softmax() 가 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4661, -1.3655, -2.1714, -1.7770, -1.4664],\n",
       "        [-1.2339, -1.2855, -1.9580, -1.9562, -1.8985],\n",
       "        [-1.6426, -1.2071, -2.0212, -1.4390, -1.9821]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비용함수의 경우 아래의 식을 통해 계산하였다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8145, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_one_hot) * - F.log_softmax(z, dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nll_loss()를 사용하면 더 간단하게 표현 가능하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8145, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nll 이란 Negative Log Likehood의 약자이다. 위에서 nll_loss는 log_softmax를 수행한 후 남은 수식을 수행한다. 이를 더 간단하게 표현 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8145, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_entropy는 비용 함수에 소프트맥스 함수까지 포함하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8145, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8145, grad_fn=<MeanBackward0>)\n",
      "tensor(1.8145, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.8145, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(((y_one_hot) * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean())\n",
    "\n",
    "# F.softmax() + torch.log() = F.log_softmax()\n",
    "print(((y_one_hot) * -F.log_softmax(z, dim=1)).sum(dim=1).mean())\n",
    "\n",
    "# F.log_softmax() + F.nll_loss() = F.cross_entropy()\n",
    "print(F.nll_loss(F.log_softmax(z, dim=1), y))\n",
    "print(F.cross_entropy(z, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
